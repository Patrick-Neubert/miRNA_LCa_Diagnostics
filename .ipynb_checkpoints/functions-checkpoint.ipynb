{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:58:17.387859Z",
     "start_time": "2020-05-07T14:58:17.379354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #1\n",
    "# Function for feature_selection\n",
    "# from Feature Importance of Tree-based Classifiers\n",
    "\n",
    "def feature_selection(results_dict, tree_clf, name_clf, X_train, y_train, n=20):\n",
    "\n",
    "    # n is the number of Features + Importances added to the results dictionary (default=20)\n",
    "    # and printed during process for each classifier\n",
    "    # if n=None all Features + Importances are added and printed\n",
    "    \n",
    "    count = 0\n",
    "    for classifier in tree_clf:\n",
    "        # use classifiers to fit data\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "        # calculate Feature Importances, rank by value and get Feature Name\n",
    "        importances = classifier.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feature_name = X_train.columns[indices]\n",
    "    \n",
    "        # Write first n Ranks to dictionary results_dict as \n",
    "        # \"name_clf: (feature_name, importance)\", default is n=20\n",
    "        results_dict[name_clf[count]] = list(zip(feature_name[0:n], importances[indices][0:n]))     \n",
    "        count +=1\n",
    "    \n",
    "        # Print the feature ranking (only the first n Ranks), default is n=20\n",
    "        print(80*\"-\") \n",
    "        print(classifier)\n",
    "        print(\"Feature ranking:\")\n",
    "        for feature in range(n) if isinstance(n, int) else range(importances.size):\n",
    "            print(\"%d. Feature: %s (%f)\" % (feature + 1, X_train.columns[indices[feature]], importances[indices[feature]]))\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:58:51.090766Z",
     "start_time": "2020-05-07T14:58:51.076569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #2\n",
    "# Function for RFE Feature Selection\n",
    "\n",
    "def rfe_selection(results_dict, non_tree_clf, nt_name_clf, X_train, y_train):\n",
    "    \n",
    "    count = 0\n",
    "    for classifier in non_tree_clf:\n",
    "        \n",
    "        #Initialize RFE Feature Selector\n",
    "        rfe_selector = RFE(estimator=classifier, n_features_to_select=1, step=1) \n",
    "        \n",
    "        # fit data\n",
    "        rfe_selector.fit(X_train, y_train) \n",
    "        \n",
    "        # get the indices of Feature Ranking\n",
    "        indices = rfe_selector.ranking_\n",
    "        \n",
    "        # Create a Lists with ordered feature names \n",
    "        # from ranking (list with ordered positions of object in DataFrame)\n",
    "        feature_name = []\n",
    "        \n",
    "        # to get the rank (index of ranking) another list is created \n",
    "        feature_rank = []\n",
    "        \n",
    "        for feature in indices:\n",
    "            \n",
    "            # [feature-1] because columns starts at index 0 = Position1 in Dataframe\n",
    "            feature_name.append(X_train.columns[feature-1])\n",
    "            \n",
    "            # make a list of indices array and get the index of the feature \n",
    "            #(+1, because index of this list starts at 0) = rank of feature\n",
    "            feature_rank.append(list(indices).index(feature)+1)\n",
    "    \n",
    "        # Write first 20 Ranks to dictionary results_dict as \n",
    "        # \"nt_name_clf: (feature_name, feature_rank)\"\n",
    "        results_dict[nt_name_clf[count]] = list(zip(feature_name[0:20], feature_rank[0:20]))     \n",
    "        count +=1\n",
    "    \n",
    "        # Print the feature ranking (only the first 20 Ranks)\n",
    "        print(80*\"-\") \n",
    "        print(classifier)\n",
    "        print(\"Feature ranking:\")\n",
    "        for feature in range(20):\n",
    "            print(\"%d. Feature: %s\" % (feature + 1, feature_name[feature]))\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:58:53.331841Z",
     "start_time": "2020-05-07T14:58:53.324026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #3\n",
    "#Function to create Dataframe with selected Features\n",
    "\n",
    "def dataframe_selection(results_fs, X_train):\n",
    "    \n",
    "    #First create a list of selected Features from Feature Selection Results\n",
    "    selected_features = [feature_name[0] for feature_name in results_fs]\n",
    "\n",
    "    #Then create exclusion list for dropping (default=X_train)\n",
    "    exclusion_list = list(set(X_train.columns) - set(selected_features))\n",
    "\n",
    "    #Now drop all from X_train except selected_features and create new dataframe\n",
    "    X_new = X_train.drop(exclusion_list, axis=1)\n",
    "    \n",
    "    return X_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:59:19.156707Z",
     "start_time": "2020-05-07T14:59:19.149235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #4\n",
    "# Function for model_evaluation\n",
    "\n",
    "def model_evaluation(results_dict, model_clf, name_model, X_train, y_train):\n",
    "    count = 0\n",
    "    for model in model_clf:\n",
    "        # use model to be cross validated with data and desired metrics\n",
    "        validation = cross_validate(model, X_train, y_train, scoring = ('accuracy', 'roc_auc_ovr', 'precision_macro', 'f1_macro'))\n",
    "\n",
    "        #get metric scores from validation dictionary and calculate mean\n",
    "        accuracy = np.mean(validation['test_accuracy'])\n",
    "        roc_auc = np.mean(validation['test_roc_auc_ovr'])\n",
    "        precision = np.mean(validation['test_precision_macro'])\n",
    "        f1 = np.mean(validation['test_f1_macro'])\n",
    "\n",
    "\n",
    "        # Write all mean scores to dictionary results_cv as \n",
    "        # \"name_model: {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\"\n",
    "        results_dict[name_model[count]] = {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\n",
    "\n",
    "        # Print the scores for each model\n",
    "        print(80*\"-\") \n",
    "        print(model)\n",
    "        print(\"Scores:\")\n",
    "        print(results_dict[name_model[count]])\n",
    "\n",
    "        count +=1\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:59:33.959671Z",
     "start_time": "2020-05-07T14:59:33.951658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #5\n",
    "# function to print the n best model + selection combinations\n",
    "# (n highest values with their index, column) for a certain\n",
    "# dataframe (default n=3)\n",
    "\n",
    "def top_model(dataframe, n=3):\n",
    "    \n",
    "    # create an numpy array of all values\n",
    "    # (as copy to ensure original dateframe is not touched)\n",
    "    values = dataframe.to_numpy(copy=True)\n",
    "    \n",
    "    #sort values to get the max per row (=model)\n",
    "    values.sort()\n",
    "    \n",
    "    # make a list of the max values for each row (=model)\n",
    "\n",
    "    top_model = []\n",
    "\n",
    "    for val in range(values.shape[0]):\n",
    "        top_model.append(values[val][-1])\n",
    "    \n",
    "    # sort the list descending to get the highest values in the whole dataframe first\n",
    "    top_model.sort(reverse=True)\n",
    "    \n",
    "    # print out the Top n Values (of all in dataframe)\n",
    "    for tm in range(n) if n<= values.shape[0] else range(values.shape[0]):\n",
    "       \n",
    "        #get index and colum name (=Model + Selection)\n",
    "        idx, clm = np.where(dataframe == top_model[tm])\n",
    "        \n",
    "        print(\"TOP:\", tm+1)\n",
    "        print(\"Value:\", top_model[tm])\n",
    "        print(\"Model:\", dataframe.index[idx][0])\n",
    "        print(\"Feature Selection:\", dataframe.columns[clm][0])\n",
    "        print(65*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:59:35.886229Z",
     "start_time": "2020-05-07T14:59:35.865200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #6\n",
    "def multibar_plot(bars, label_list, name, title, xlb, barWidth = 0.1):\n",
    "     \n",
    "    #gives an array with indeces of dataframe @ postion [0] in bars list\n",
    "    r1 = np.arange(len(bars[0]))\n",
    "    \n",
    "    # Set position of other bars relative to r1 on X axis and save it in position_list\n",
    "    # set barWidth = 0.1 by default\n",
    "    position_list = []\n",
    "    for ps in list(range(1, len(bars))):\n",
    "        r2 = [x + barWidth*ps for x in r1]\n",
    "        position_list.append(r2)\n",
    "\n",
    "    #set Size of figure\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    # Make the plot\n",
    "    plt.bar(r1, bars[0], width=barWidth, edgecolor='white', label=label_list[0])\n",
    "    \n",
    "    for num in list(range(1, len(bars))):\n",
    "        plt.bar(position_list[num-1], bars[num], width=barWidth, edgecolor='white', label=label_list[num])\n",
    "        \n",
    "    # wenn weniger labels als bars dann geht es nicht!\n",
    "        \n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel(xlb, fontweight='bold') #, fontsize='large')\n",
    "    \n",
    "    # to set ticks on the middle bar\n",
    "    plt.xticks([r + barWidth*(int((len(label_list))/2)) for r in range(len(bars[0]))], name)\n",
    " \n",
    "    # Create legend & Show graphic\n",
    "    plt.legend(loc='best', bbox_to_anchor=(0.46, 0.0, 0.7 ,0.36)) #, fontsize='medium')\n",
    "    plt.title(label=title, fontweight='bold') #, fontsize='large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T14:59:49.904145Z",
     "start_time": "2020-05-07T14:59:49.880985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #7\n",
    "#function for RandomSearchCV + printing results\n",
    "\n",
    "def random_searching(model, parameters, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Perform Random search on the classifier using 'precision_micro' as the scoring method \n",
    "    #(micro = Calculate metrics globally by counting the total true positives, false negatives and false positives.)\n",
    "    random_obj = RandomizedSearchCV(model, parameters, scoring='precision_micro', n_jobs = -1, verbose=5, n_iter=100, cv=5, random_state=seed)\n",
    "\n",
    "    # Fit the Random search object to the training data and find the optimal parameters\n",
    "    random_fit = random_obj.fit(X_train, y_train)\n",
    "    \n",
    "    #Fit the unoptimzed model\n",
    "    model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "    # Get the estimators\n",
    "    best_model = random_fit.best_estimator_\n",
    "\n",
    "    # Make predictions using the unoptimized and optimized model\n",
    "    predictions = model_fit.predict(X_test)\n",
    "    best_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    probabilities = model_fit.predict_proba(X_test)\n",
    "    best_probabilities = best_model.predict_proba(X_test)\n",
    "    \n",
    "    #get all the metrics of optimized and unoptimized model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_best = accuracy_score(y_test, best_predictions)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, probabilities, multi_class='ovr')\n",
    "    roc_auc_best = roc_auc_score(y_test, best_probabilities, multi_class='ovr')\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average='micro')\n",
    "    precision_best = precision_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    f1 = f1_score(y_test, predictions, average='micro')\n",
    "    f1_best = f1_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    # Report the before-and-afterscores\n",
    "    print(\"Unoptimized model\\n------\")\n",
    "    print(\"Accuracy score Unoptimized:\", accuracy)\n",
    "    print(\"Roc_AUC score Unoptimized:\", roc_auc)\n",
    "    print(\"Precision score Unoptimized:\", precision)\n",
    "    print(\"F1 score Unoptimized:\", f1)\n",
    "   \n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Accuracy score Optimized:\", accuracy_best)\n",
    "    print(\"Roc_AUC score Optimized:\", roc_auc_best)\n",
    "    print(\"Precision score Optimized:\", precision_best)\n",
    "    print(\"F1 score Optimized:\", f1_best)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:00:08.316793Z",
     "start_time": "2020-05-07T15:00:08.301891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #8\n",
    "#function for GridSearchCV + printing and saving results results\n",
    "\n",
    "def grid_searching(results_dict, model, name_model, parameters, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Perform grid search on the classifier using 'precision_micro' as the scoring method \n",
    "    #(micro = Calculate metrics globally by counting the total true positives, false negatives and false positives.)\n",
    "    grid_obj = GridSearchCV(model, parameters, scoring='precision_micro', n_jobs = -1, verbose=5, cv=5)\n",
    "\n",
    "    # Fit the grid search object to the training data and find the optimal parameters\n",
    "    grid_fit = grid_obj.fit(X_train, y_train)\n",
    "    \n",
    "    #Fit the unoptimzed model\n",
    "    model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "    # Get the estimators\n",
    "    best_model = grid_fit.best_estimator_\n",
    "\n",
    "    # Make predictions using the unoptimized and optimized model\n",
    "    predictions = model_fit.predict(X_test)\n",
    "    best_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    probabilities = model_fit.predict_proba(X_test)\n",
    "    best_probabilities = best_model.predict_proba(X_test)\n",
    "    \n",
    "    #get all the metrics of optimized and unoptimized model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_best = accuracy_score(y_test, best_predictions)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, probabilities, multi_class='ovr')\n",
    "    roc_auc_best = roc_auc_score(y_test, best_probabilities, multi_class='ovr')\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average='micro')\n",
    "    precision_best = precision_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    f1 = f1_score(y_test, predictions, average='micro')\n",
    "    f1_best = f1_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    \n",
    "    # Write all mean scores to dictionary results_dict as \n",
    "    # \"name_model: {\"Accuracy\":accuracy_best,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\"\n",
    "    results_dict[name_model[0]] = {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\n",
    "    results_dict[name_model[1]] = {\"Accuracy\":accuracy_best,\"Roc_AUC\":roc_auc_best,\"Precision\":precision_best,\"F1\":f1_best}\n",
    "    \n",
    "    # Report the before-and-afterscores\n",
    "    print(\"Unoptimized model\\n------\")\n",
    "    print(\"Accuracy score Unoptimized:\", accuracy)\n",
    "    print(\"Roc_AUC score Unoptimized:\", roc_auc)\n",
    "    print(\"Precision score Unoptimized:\", precision)\n",
    "    print(\"F1 score Unoptimized:\", f1)\n",
    "   \n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Accuracy score Optimized:\", accuracy_best)\n",
    "    print(\"Roc_AUC score Optimized:\", roc_auc_best)\n",
    "    print(\"Precision score Optimized:\", precision_best)\n",
    "    print(\"F1 score Optimized:\", f1_best)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:00:18.093345Z",
     "start_time": "2020-05-07T15:00:18.088588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #9\n",
    "def viz_summary(results_dict, metric):\n",
    "    print(metric, \"Value:\")\n",
    "    print(results_dict.T[metric])\n",
    "    print(40*\"-\")\n",
    "    print(metric, \"Rank:\")\n",
    "    print(results_dict.T[metric].rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:00:30.865193Z",
     "start_time": "2020-05-07T15:00:30.858711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #10\n",
    "def feature_reduce(results_reduction, model_reduction, name_list, feature_list, elim_features, X_train, n=11):\n",
    "        \n",
    "    name_reduction = name_list.copy()\n",
    "    feature_reduction = feature_list.copy()\n",
    "\n",
    "    # set default for n=11 iterations =(From All20 to TOP10)\n",
    "    for n in range(n):\n",
    "        \n",
    "        # Create exclusion list for dropping (default=X_train)\n",
    "        exclusion_list = list(set(X_train.columns) - set(feature_reduction))\n",
    "        \n",
    "        #Now drop all from X_train except feature_reduction and create new dataframe (first iteration all)\n",
    "        X_reduce = X_train.drop(exclusion_list, axis=1)\n",
    "        \n",
    "       \n",
    "        # Use model_evaluation function with reduced Features Dataset\n",
    "        model_evaluation(results_reduction, model_reduction, name_reduction, X_reduce, y_train)\n",
    "    \n",
    "        #return the list with features used (only for debugging)\n",
    "        elim_features.append(X_reduce.columns)\n",
    "    \n",
    "        # reduce feature list by 1 Feature\n",
    "        feature_reduction.pop()\n",
    "    \n",
    "        # get new key for results dictionary\n",
    "        name_reduction.pop(0)\n",
    "    \n",
    "    return results_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:00:41.012250Z",
     "start_time": "2020-05-07T15:00:41.007030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #11\n",
    "# Function for plottin Classification Report and final Precision\n",
    "def score_eval(y_test, y_pred):\n",
    "    # precision micro = Calculate metrics globally by counting the total true positives,\n",
    "    # false negatives and false positives.\n",
    "    score = precision_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print('Final Precision Score (micro):', score)\n",
    "    print('----' * 15)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('----' * 15)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:00:51.120871Z",
     "start_time": "2020-05-07T15:00:51.116113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #12\n",
    "# function to print heatmap\n",
    "\n",
    "def heatmap(correlation, name_savefig):\n",
    "\n",
    "    # Create Correlation Heatmap for all values (with minimum value -1 for positive or negative correlation)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    heatmap = sns.heatmap(correlation, vmin=-1, cmap=\"seismic\", annot=True) #inferno, seismic, magma, icefire\n",
    "    \n",
    "    # save figure\n",
    "    plt.savefig(name_savefig, transparent=True, dpi=200)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
