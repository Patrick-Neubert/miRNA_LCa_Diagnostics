{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for project: \"miRNA Biomarker for Lung Cancer Diagnostics - Selecting a test panel for patient classification -\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Packages & Modules\n",
    "\n",
    "# IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "# mlxtend\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz  \n",
    "\n",
    "# subprocess\n",
    "from subprocess import call\n",
    "\n",
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# yellowbrick\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# missingno\n",
    "import missingno\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# pickle (for saving and loading data)\n",
    "import pickle # \n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# sys (enables to exit execution of code)\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.069881Z",
     "start_time": "2020-05-08T08:36:57.050450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #1\n",
    "# Function for feature_selection from Feature Importance of Tree-based Classifiers\n",
    "\n",
    "def feature_selection(results_dict, tree_clf, name_clf, X_train, y_train, n=20):\n",
    "    \n",
    "    # n is the number of Features + Importances added to the results dictionary (default=20)\n",
    "    # and printed during process for each classifier\n",
    "    # if n=None all Features + Importances are added and printed\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for classifier in tree_clf:\n",
    "        # use classifiers to fit data\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "        # calculate Feature Importances, rank by value and get Feature Name\n",
    "        importances = classifier.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        feature_name = X_train.columns[indices]\n",
    "    \n",
    "        # Write first n Ranks to dictionary results_dict as \n",
    "        # \"name_clf: (feature_name, importance)\", default is n=20\n",
    "        results_dict[name_clf[count]] = list(zip(feature_name[0:n], importances[indices][0:n]))     \n",
    "        \n",
    "        count +=1\n",
    "    \n",
    "        # Print the feature ranking (only the first n Ranks), default is n=20\n",
    "        print(80*\"-\") \n",
    "        print(classifier)\n",
    "        print(\"Feature ranking:\")\n",
    "        for feature in range(n) if isinstance(n, int) else range(importances.size):\n",
    "            print(\"%d. Feature: %s (%f)\" % (feature + 1, X_train.columns[indices[feature]], importances[indices[feature]]))\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.191559Z",
     "start_time": "2020-05-08T08:36:57.096907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #2\n",
    "# Function for RFE Feature Selection\n",
    "\n",
    "def rfe_selection(results_dict, non_tree_clf, nt_name_clf, X_train, y_train):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for classifier in non_tree_clf:\n",
    "        \n",
    "        # Initialize RFE Feature Selector\n",
    "        rfe_selector = RFE(estimator=classifier, n_features_to_select=1, step=1) \n",
    "        \n",
    "        # fit data\n",
    "        rfe_selector.fit(X_train, y_train) \n",
    "        \n",
    "        # get the indices of Feature Ranking\n",
    "        indices = rfe_selector.ranking_\n",
    "        \n",
    "        # Create a Lists with ordered feature names \n",
    "        # from ranking (list with ordered positions of object in DataFrame)\n",
    "        feature_name = []\n",
    "        \n",
    "        # to get the rank (index of ranking) another list is created \n",
    "        feature_rank = []\n",
    "        \n",
    "        for feature in indices:\n",
    "            \n",
    "            # [feature-1] because columns starts at index 0 = Position1 in Dataframe\n",
    "            feature_name.append(X_train.columns[feature-1])\n",
    "            \n",
    "            # make a list of indices array and get the index of the feature \n",
    "            # (+1, because index of this list starts at 0) = rank of feature\n",
    "            feature_rank.append(list(indices).index(feature)+1)\n",
    "    \n",
    "        # Write first 20 Ranks to dictionary results_dict as \n",
    "        # \"nt_name_clf: (feature_name, feature_rank)\"\n",
    "        results_dict[nt_name_clf[count]] = list(zip(feature_name[0:20], feature_rank[0:20]))     \n",
    "        \n",
    "        count +=1\n",
    "    \n",
    "        # Print the feature ranking (only the first 20 Ranks)\n",
    "        print(80*\"-\") \n",
    "        print(classifier)\n",
    "        print(\"Feature ranking:\")\n",
    "        for feature in range(20):\n",
    "            print(\"%d. Feature: %s\" % (feature + 1, feature_name[feature]))\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.227019Z",
     "start_time": "2020-05-08T08:36:57.199036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #3\n",
    "# Function to create Dataframe with selected Features\n",
    "\n",
    "def dataframe_selection(results_fs, X_train):\n",
    "    \n",
    "    # First create a list of selected Features from Feature Selection Results\n",
    "    selected_features = [feature_name[0] for feature_name in results_fs]\n",
    "\n",
    "    # Then create exclusion list for dropping (default=X_train)\n",
    "    exclusion_list = list(set(X_train.columns) - set(selected_features))\n",
    "\n",
    "    # Now drop all from X_train except selected_features and create new dataframe\n",
    "    X_new = X_train.drop(exclusion_list, axis=1)\n",
    "    \n",
    "    return X_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.262520Z",
     "start_time": "2020-05-08T08:36:57.243686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #4\n",
    "# Function for model_evaluation\n",
    "\n",
    "def model_evaluation(results_dict, model_clf, name_model, X_train, y_train):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for model in model_clf:\n",
    "        # use model to be cross validated with data and desired metrics\n",
    "        validation = cross_validate(model, X_train, y_train, scoring = ('accuracy', 'roc_auc_ovr', 'precision_macro', 'f1_macro'))\n",
    "\n",
    "        # get metric scores from validation dictionary and calculate mean\n",
    "        accuracy = np.mean(validation['test_accuracy'])\n",
    "        roc_auc = np.mean(validation['test_roc_auc_ovr'])\n",
    "        precision = np.mean(validation['test_precision_macro'])\n",
    "        f1 = np.mean(validation['test_f1_macro'])\n",
    "\n",
    "\n",
    "        # Write all mean scores to dictionary results_cv as \n",
    "        # \"name_model: {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\"\n",
    "        results_dict[name_model[count]] = {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\n",
    "\n",
    "        # Print the scores for each model\n",
    "        print(80*\"-\") \n",
    "        print(model)\n",
    "        print(\"Scores:\")\n",
    "        print(results_dict[name_model[count]])\n",
    "\n",
    "        count +=1\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.282649Z",
     "start_time": "2020-05-08T08:36:57.270279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #5\n",
    "# Function to print the n best model + selection combinations (n highest values with their index, column) \n",
    "# for a certain dataframe (default n=3)\n",
    "\n",
    "def top_model(dataframe, n=3):\n",
    "    \n",
    "    # create an numpy array of all values\n",
    "    # (as copy to ensure original dateframe is not touched)\n",
    "    values = dataframe.to_numpy(copy=True)\n",
    "    \n",
    "    # sort values to get the max per row (=model)\n",
    "    values.sort()\n",
    "    \n",
    "    # make a list of the max values for each row (=model)\n",
    "\n",
    "    top_model = []\n",
    "\n",
    "    for val in range(values.shape[0]):\n",
    "        top_model.append(values[val][-1])\n",
    "    \n",
    "    # sort the list descending to get the highest values in the whole dataframe first\n",
    "    top_model.sort(reverse=True)\n",
    "    \n",
    "    # print out the Top n Values (of all in dataframe)\n",
    "    for tm in range(n) if n<= values.shape[0] else range(values.shape[0]):\n",
    "       \n",
    "        # get index and colum name (=Model + Selection)\n",
    "        idx, clm = np.where(dataframe == top_model[tm])\n",
    "        \n",
    "        print(\"TOP:\", tm+1)\n",
    "        print(\"Value:\", top_model[tm])\n",
    "        print(\"Model:\", dataframe.index[idx][0])\n",
    "        print(\"Feature Selection:\", dataframe.columns[clm][0])\n",
    "        print(65*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.304076Z",
     "start_time": "2020-05-08T08:36:57.286131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #6\n",
    "# Function for multibar plotting\n",
    "\n",
    "def multibar_plot(bars, label_list, name, title, xlb, barWidth = 0.1):\n",
    "    \n",
    "    # gives an array with indeces of dataframe @ postion [0] in bars list\n",
    "    r1 = np.arange(len(bars[0]))\n",
    "    \n",
    "    # Set position of other bars relative to r1 on X axis and save it in position_list\n",
    "    # Set barWidth = 0.1 by default\n",
    "    position_list = []\n",
    "    for ps in list(range(1, len(bars))):\n",
    "        r2 = [x + barWidth*ps for x in r1]\n",
    "        position_list.append(r2)\n",
    "\n",
    "    # Make the plot\n",
    "    plt.bar(r1, bars[0], width=barWidth, edgecolor='white', label=label_list[0])\n",
    "    \n",
    "    for num in list(range(1, len(bars))):\n",
    "        plt.bar(position_list[num-1], bars[num], width=barWidth, edgecolor='white', label=label_list[num])\n",
    "        \n",
    "    # doesn´t work if less labels than bars! make sure labels list is complete!\n",
    "        \n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel(xlb, fontweight='bold') #, fontsize='large')\n",
    "    \n",
    "    # to set ticks on the middle bar\n",
    "    plt.xticks([r + barWidth*(int((len(label_list))/2)) for r in range(len(bars[0]))], name)\n",
    " \n",
    "    # Create legend & Show graphic\n",
    "    plt.legend(loc='best', bbox_to_anchor=(0.46, 0.0, 0.7 ,0.36)) #, fontsize='medium')\n",
    "    plt.title(label=title, fontweight='bold', fontsize='large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.325748Z",
     "start_time": "2020-05-08T08:36:57.307001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #7\n",
    "# Function for RandomSearchCV + printing results\n",
    "\n",
    "def random_searching(model, parameters, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Perform Random search on the classifier using 'precision_micro' as the scoring method \n",
    "    #(micro = Calculate metrics globally by counting the total true positives, false negatives and false positives.)\n",
    "    random_obj = RandomizedSearchCV(model, parameters, scoring='precision_micro', n_jobs = -1, verbose=5, n_iter=100, cv=5, random_state=seed)\n",
    "\n",
    "    # Fit the Random search object to the training data and find the optimal parameters\n",
    "    random_fit = random_obj.fit(X_train, y_train)\n",
    "    \n",
    "    # Fit the unoptimzed model\n",
    "    model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "    # Get the estimators\n",
    "    best_model = random_fit.best_estimator_\n",
    "\n",
    "    # Make predictions using the unoptimized and optimized model\n",
    "    predictions = model_fit.predict(X_test)\n",
    "    best_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    probabilities = model_fit.predict_proba(X_test)\n",
    "    best_probabilities = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # get all the metrics of optimized and unoptimized model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_best = accuracy_score(y_test, best_predictions)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, probabilities, multi_class='ovr')\n",
    "    roc_auc_best = roc_auc_score(y_test, best_probabilities, multi_class='ovr')\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average='micro')\n",
    "    precision_best = precision_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    f1 = f1_score(y_test, predictions, average='micro')\n",
    "    f1_best = f1_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    # Report the before-and-afterscores\n",
    "    print(\"Unoptimized model\\n------\")\n",
    "    print(\"Accuracy score Unoptimized:\", accuracy)\n",
    "    print(\"Roc_AUC score Unoptimized:\", roc_auc)\n",
    "    print(\"Precision score Unoptimized:\", precision)\n",
    "    print(\"F1 score Unoptimized:\", f1)\n",
    "   \n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Accuracy score Optimized:\", accuracy_best)\n",
    "    print(\"Roc_AUC score Optimized:\", roc_auc_best)\n",
    "    print(\"Precision score Optimized:\", precision_best)\n",
    "    print(\"F1 score Optimized:\", f1_best)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.348033Z",
     "start_time": "2020-05-08T08:36:57.335105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #8\n",
    "# Function for GridSearchCV + printing and saving results\n",
    "\n",
    "def grid_searching(results_dict, model, name_model, parameters, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Perform grid search on the classifier using 'precision_micro' as the scoring method \n",
    "    # (micro = Calculate metrics globally by counting the total true positives, false negatives and false positives.)\n",
    "    grid_obj = GridSearchCV(model, parameters, scoring='precision_micro', n_jobs = -1, verbose=5, cv=5)\n",
    "\n",
    "    # Fit the grid search object to the training data and find the optimal parameters\n",
    "    grid_fit = grid_obj.fit(X_train, y_train)\n",
    "    \n",
    "    # Fit the unoptimzed model\n",
    "    model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "    # Get the estimators\n",
    "    best_model = grid_fit.best_estimator_\n",
    "\n",
    "    # Make predictions using the unoptimized and optimized model\n",
    "    predictions = model_fit.predict(X_test)\n",
    "    best_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    probabilities = model_fit.predict_proba(X_test)\n",
    "    best_probabilities = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # get all the metrics of optimized and unoptimized model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_best = accuracy_score(y_test, best_predictions)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, probabilities, multi_class='ovr')\n",
    "    roc_auc_best = roc_auc_score(y_test, best_probabilities, multi_class='ovr')\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average='micro')\n",
    "    precision_best = precision_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    f1 = f1_score(y_test, predictions, average='micro')\n",
    "    f1_best = f1_score(y_test, best_predictions, average='micro')\n",
    "    \n",
    "    \n",
    "    # Write all mean scores to dictionary results_dict as \n",
    "    # \"name_model: {\"Accuracy\":accuracy_best,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\"\n",
    "    results_dict[name_model[0]] = {\"Accuracy\":accuracy,\"Roc_AUC\":roc_auc,\"Precision\":precision,\"F1\":f1}\n",
    "    results_dict[name_model[1]] = {\"Accuracy\":accuracy_best,\"Roc_AUC\":roc_auc_best,\"Precision\":precision_best,\"F1\":f1_best}\n",
    "    \n",
    "    # Report the before-and-afterscores\n",
    "    print(\"Unoptimized model\\n------\")\n",
    "    print(\"Accuracy score Unoptimized:\", accuracy)\n",
    "    print(\"Roc_AUC score Unoptimized:\", roc_auc)\n",
    "    print(\"Precision score Unoptimized:\", precision)\n",
    "    print(\"F1 score Unoptimized:\", f1)\n",
    "   \n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Accuracy score Optimized:\", accuracy_best)\n",
    "    print(\"Roc_AUC score Optimized:\", roc_auc_best)\n",
    "    print(\"Precision score Optimized:\", precision_best)\n",
    "    print(\"F1 score Optimized:\", f1_best)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.363383Z",
     "start_time": "2020-05-08T08:36:57.353856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #9\n",
    "# Function to print Value and Rank of certain metric in a results dictionary\n",
    "\n",
    "def viz_summary(results_dict, metric):\n",
    "    print(metric, \"Value:\")\n",
    "    print(results_dict.T[metric])\n",
    "    print(40*\"-\")\n",
    "    print(metric, \"Rank:\")\n",
    "    print(results_dict.T[metric].rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.375155Z",
     "start_time": "2020-05-08T08:36:57.367142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #10\n",
    "# Function for recursive feature reduction\n",
    "\n",
    "def feature_reduce(results_reduction, model_reduction, name_list, feature_list, elim_features, X_train, y_train, n=11):\n",
    "        \n",
    "    name_reduction = name_list.copy()\n",
    "    feature_reduction = feature_list.copy()\n",
    "\n",
    "    # set default for n=11 iterations =(From All20 to TOP10)\n",
    "    for n in range(n):\n",
    "        \n",
    "        # Create exclusion list for dropping (default=X_train)\n",
    "        exclusion_list = list(set(X_train.columns) - set(feature_reduction))\n",
    "        \n",
    "        # Now drop all from X_train except feature_reduction and create new dataframe (first iteration all)\n",
    "        X_reduce = X_train.drop(exclusion_list, axis=1)\n",
    "        \n",
    "       \n",
    "        # Use model_evaluation function with reduced Features Dataset\n",
    "        model_evaluation(results_reduction, model_reduction, name_reduction, X_reduce, y_train)\n",
    "    \n",
    "        # return the list with features used (only for debugging)\n",
    "        elim_features.append(X_reduce.columns)\n",
    "    \n",
    "        # reduce feature list by 1 Feature\n",
    "        feature_reduction.pop()\n",
    "    \n",
    "        # get new key for results dictionary\n",
    "        name_reduction.pop(0)\n",
    "    \n",
    "    return results_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:36:57.389927Z",
     "start_time": "2020-05-08T08:36:57.382956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #11\n",
    "# Function for plotting Classification Report and Final Precision\n",
    "\n",
    "def score_eval(y_test, y_pred):\n",
    "    \n",
    "    # precision micro = Calculate metrics globally by counting the total true positives,\n",
    "    # false negatives and false positives.\n",
    "    score = precision_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print('Final Precision Score (micro):', score)\n",
    "    print('----' * 15)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('----' * 15)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T16:36:14.202170Z",
     "start_time": "2020-05-08T16:36:14.198023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function #12\n",
    "# Function to print Heatmap\n",
    "\n",
    "def heatmap(correlation, name_savefig, cmap):\n",
    "\n",
    "    # Create Correlation Heatmap for all values (with minimum value -1 for positive or negative correlation)\n",
    "    # alternative cmaps: inferno, seismic, magma, icefire\n",
    "    plt.figure(figsize=(20,20))\n",
    "    heatmap = sns.heatmap(correlation, vmin=-1, cmap=cmap, annot=True)\n",
    "    \n",
    "    # save figure\n",
    "    plt.savefig(name_savefig, transparent=True, dpi=300)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
